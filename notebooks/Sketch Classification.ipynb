{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketch Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This notebook is inspired by Google's Quick Draw Project: \n",
    "- https://quickdraw.withgoogle.com/\n",
    "Data source: \n",
    "- https://github.com/googlecreativelab/quickdraw-dataset\n",
    "The model is deployed at:\n",
    "- http://35.237.138.188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from memory_map import MemoryMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = './train_simplified'\n",
    "N_LABEL_SAMPLE = 50000\n",
    "VALIDATION_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_offsets():\n",
    "    files = os.listdir(INPUT_PATH)\n",
    "    return [f.split('.')[0] for f in files if re.search('\\.offsets$', f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = read_offsets()\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memmaps = []\n",
    "for index, filename in enumerate(filenames):\n",
    "    memmaps.append(MemoryMap(INPUT_PATH, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for index, filename in enumerate(filenames):\n",
    "    file_metadata_path = os.path.join(INPUT_PATH, filename + \".offsets\")\n",
    "    with open (file_metadata_path, 'rb') as fp:\n",
    "        offsets = pickle.load(fp)\n",
    "        metadata.extend([(index,) + offset for offset in offsets[:N_LABEL_SAMPLE]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line(line_pointer):\n",
    "    (file_index, start, end) = line_pointer\n",
    "    return memmaps[file_index].memmap[start:end-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = metadata[:]\n",
    "random.shuffle(shuffled)\n",
    "read_line(shuffled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_offsets = shuffled[:-VALIDATION_SIZE]\n",
    "val_offsets = shuffled[-VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1988)\n",
    "tf.set_random_seed(1988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = 340\n",
    "STEPS = 20\n",
    "SIZE = 64\n",
    "BASE_SIZE = 256\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried different drawing techniques such as creating rgb drawing where each stroke has a unique color and creating a grayscale drawing where the first stroke has black and it gets lighter as you go through more strokes. The RGB drawings took much longer to train and didn't have much affect on the accuracy, so I decided to work with grayscale drawings. The below function does something similar to the stroke by stroke grayscale drawing, but the color gets lighter as you go through pixels. This method gave me the best results.\n",
    "\n",
    "I am using the simplified version of the training data where the time information is stripped out and the strokes are simplified. Here is the process of simplification: \n",
    "\n",
    "1- Strip time information\n",
    "2- Align the drawing to the top-left corner, to have minimum values of 0\n",
    "3- Uniformly scale the drawing, to have a maximum value of 255\n",
    "4- Resample all strokes with a 1 pixel spacing\n",
    "5- Simplify all strokes using the Ramer–Douglas–Peucker algorithm with an epsilon value of 2.0\n",
    "\n",
    "The accuracy of the model could be improved slightly if I used the time information and draw the images using the time information. However, the strokes are ordered in the simplified version as well. Using the pixel ordering as \"time\" information gave pretty good results. You can manually test the results on the website I deployed for sketch classification. http://35.237.138.188 The model can predict the sketches pretty early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/hasibzunair/5ea9009a28e4c5a8e6e44bafe6ba4104\n",
    "def draw(raw_strokes, size=256, lw=6, time_color=True):\n",
    "    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "    for t, stroke in enumerate(raw_strokes):\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            color = 255 - min(t, 10) * 13 if time_color else 255\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n",
    "    if size != BASE_SIZE:\n",
    "        return cv2.resize(img, (size, size))\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(size, batchsize, lw=6, time_color=True):\n",
    "    while True:\n",
    "        for i in range(len(train_offsets) // batchsize + (len(train_offsets) % batchsize > 0)):\n",
    "            batch = train_offsets[i*batchsize:min((i+1)*batchsize,len(train_offsets))]\n",
    "            x = np.zeros((batchsize, size, size, 1))\n",
    "            y = []\n",
    "            for i in range(batchsize):\n",
    "                line = read_line(batch[i]).decode(encoding=\"utf-8\")\n",
    "                if line != '':\n",
    "                    raw_strokes = ast.literal_eval(re.findall(r'\"(.*?)\"', line)[0])\n",
    "                    x[i, :, :, 0] = draw(raw_strokes, size=size, lw=6, time_color=True)\n",
    "                    y.append(line.rsplit(',', 1)[1])\n",
    "            x = preprocess_input(x).astype(np.float32)\n",
    "            y = to_categorical(label_encoder.transform(y), num_classes=N_LABELS)\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_valid = np.zeros((VALIDATION_SIZE, SIZE, SIZE, 1))\n",
    "y = []\n",
    "for i in range(VALIDATION_SIZE):\n",
    "    line = read_line(val_offsets[i]).decode(encoding=\"utf-8\")\n",
    "    if line != '':\n",
    "        raw_strokes = ast.literal_eval(re.findall(r'\"(.*?)\"', line)[0])\n",
    "        x_valid[i, :, :, 0] = draw(raw_strokes, size=SIZE, lw=6, time_color=True)\n",
    "        y.append(line.rsplit(',', 1)[1])  \n",
    "x_valid = preprocess_input(x_valid).astype(np.float32)\n",
    "y_valid = to_categorical(label_encoder.transform(y), num_classes=N_LABELS)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = image_generator(size=SIZE, batchsize=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x, y = next(train_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(input_shape=(SIZE, SIZE, 1), alpha=1., weights=None, classes=N_LABELS)\n",
    "model.compile(optimizer=Adam(lr=0.0015), loss='categorical_crossentropy',\n",
    "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.75, patience=5, min_delta=0.001, mode='max', min_lr=1e-5, verbose=1),\n",
    "    ModelCheckpoint('model.h5', monitor='val_top_3_accuracy', mode='max', save_best_only=True, save_weights_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hists = []\n",
    "hist = model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS, verbose=1,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks = callbacks\n",
    ")\n",
    "hists.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_df = pd.concat([pd.DataFrame(hist.history) for hist in hists], sort=True)\n",
    "hist_df.index = np.arange(1, len(hist_df)+1)\n",
    "fig, axs = plt.subplots(nrows=1, sharex=True, figsize=(16, 10))\n",
    "axs.plot(hist_df.val_categorical_accuracy, lw=5, label='Validation Accuracy')\n",
    "axs.plot(hist_df.categorical_accuracy, lw=5, label='Training Accuracy')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.set_xlabel('Epoch')\n",
    "axs.grid()\n",
    "axs.legend(loc=0)\n",
    "fig.savefig('Accuracy.png', dpi=500)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.concat([pd.DataFrame(hist.history) for hist in hists], sort=True)\n",
    "hist_df.index = np.arange(1, len(hist_df)+1)\n",
    "fig, axs = plt.subplots(nrows=1, sharex=True, figsize=(16, 10))\n",
    "axs.plot(hist_df.val_categorical_crossentropy, lw=4, label='Validation MLogLoss')\n",
    "axs.plot(hist_df.categorical_crossentropy, lw=4, label='Training MLogLoss')\n",
    "axs.set_ylabel('MLogLoss')\n",
    "axs.set_xlabel('Epoch')\n",
    "axs.grid()\n",
    "axs.legend(loc=0)\n",
    "fig.savefig('MLogLoss.png', dpi=500)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
